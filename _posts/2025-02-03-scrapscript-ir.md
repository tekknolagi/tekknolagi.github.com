---
title: A compiler IR for Scrapscript
layout: post
---

I wrote previously about different slices of life in the various
implementations of the Scrapscript programming language. Until two weeks ago,
the most interesting implementation was a so-called ["baseline
compiler"](/blog/scrapscript-baseline/) directly from the AST to C. Now there
we have an intermediate representation---an IR.

## Why add an IR?

The AST is fine. It's a very abstract representation of the progam and it is
easy to generate directly from the source text. None of the names are
resolved (there are a bunch of strings everywhere), there is a lot left
implicit in the representation, and evaluating it requires having a stack or
recursion to keep temporary data around. But hey, it works and it's reasonably
small.

It's possible to incrementally rewrite the AST by adding new types of nodes or
folding existing ones together. For example, it's possible to find patterns
similar to `Binop(BinopKind.ADD, Int(2), Int(3))` and rewrite them directly
into `Int(5)`. This kind of optimization either doesn't go very far or also
requires keeping significant amounts of context on the side.

It's also possible to try and retrofit some of this constant folding into other
AST passes that run anyway. You can kind of see this in the post about the
[compiler tricks](/blog/scrapscript-tricks/) where we opportunistically fold
constant data into its own section so it does not need to be heap allocated.

Instead, I chose to create a new program representation designed to be
optimized and rewritten: an SSA IR. True to the spirit of the project, it's a
home-grown one, not some existing library like LLVM.

This is partly because it keeps things simple---the IR, some optimization
passes, and the compiler from the IR to C fit neatly into ~1000 lines of
code---and partly because projects like LLVM are not designed for languages
like this. They can probably do some impressive things, but they would do
better if purpose-built domain-specific analyses unwrapped some of the
functional abstractions first.

Still, you might be wondering why we then compile our IR to C instead of LLVM.
Two reasons: 1) we already have an existing C runtime and 2) I really, really,
really do not want to be tied to a particular LLVM version and its quirks. One
of the most frustrating things when hacking on someone else's language project
is watching it download a huge LLVM bundle and then compile it. It does a heck
of a lot. It's very impressive. It's many person-lifetimes of engineering
marvels. But it's too big for this dinky little tugboat of a project.

(What about QBE? Well, if I can figure out how to integrate our GC and its
notion of handles into a QBE IR, great. But for now it's C only.)

## What does the IR look like?

### A small example

Let's start small and make a list. The Scrapscript expression `[1, 2]`, when
evaluated, creates a list containing two elements. The language doesn't
expressly guarantee how lists are stored in memory but the baseline compiler
and C runtime store them as cons cells---as linked lists. The IR to build up
this linked list looks like this[^cinder]:

[^cinder]: If you're sitting there saying to yourself, "Golly gee, that looks a
    lot like the [Cinder](https://github.com/facebookincubator/cinder) IR," a)
    I am impressed you put two and two together based on vibes and b) yeah, I
    spent a lot of time reading that particular text representation so I am
    trying to mimic the style here. The actual design decisions in the IR data
    structures are pretty different.

```
fn0 {
  bb0 {
    v0 = Const<[]>
    v1 = Const<2>
    v2 = ListCons v1, v0
    v3 = Const<1>
    v4 = ListCons v3, v2
    Return v4
  }
}
```

This is a text representation generated by a `to_string` function. The actual
IR involves classes and pointers and data structures, but I sometimes find it
helpful to think in "text space".

The IR contains one function, `fn0`, which has one basic block, `bb0`, which
has a number of instructions. Each instruction is given a name which also
corresponds to the value it produces.

The first two are the `Const` instruction and they carry constant data known to
the compiler at compile-time. These are represented by "immediates" enclosed in
`<`angle brackets`>`.

The third is the `ListCons` instruction. This corresponds to a heap allocation:
building a new linked list node at run-time. It takes two operands, `v1` and
`v0`, which represent the head and tail of the new linked list node,
respectively.

The last one is the `Return` instruction, which exits the current function with
the given operand. `Return` is a control flow instruction. Control flow is only
allowed to happen at the end of a basic block, which makes all the instructions
inside a block nicely control flow free.

### A bigger example

Let's do a slightly more complicated example that involves pattern matching.
We'll take a look at the IR corresponding to the Scrapscript program `| 1 -> 2 + 3`,
which is a pattern matching function. If the argument to the function is equal
to 1, it returns 5. Otherwise, it aborts.

We'll first look at the function that contains---that creates---the pattern
matching function. The "main function", if you will.

```
fn0 {
  bb0 {
    v0 = NewClosure<fn1>
    Return v0
  }
}
```

This function allocates a closure object on the heap corresponding to the
function `fn1`. we'll come back to the details later, but the important thing
to know is that unlike in the source program and AST, `fn1` is not a name to be
looked up later as a string. Instead, it's a direct pointer to a known IR
function. And that function's IR looks like this:

```
fn1 {
  bb0 {
    v0 = Param<0; $clo>
    v1 = Param<1; arg_0>
    Jump bb2
  }
  bb2 {
    v2 = IsIntEqualWord v1, 1
    CondBranch v2, bb3, bb1
  }
  bb1 {
    MatchFail
  }
  bb3 {
    v3 = Const<2>
    v4 = Const<3>
    v5 = IntAdd v3, v4
    Return v5
  }
}
```

Whoa, multiple blocks! Let's go through them one by one.

The first block contains instructions that exist to give names to function
parameters. It might be a little confusing that we have not one but *two*
parameters for a function that has no parameters visible in the program text.
Here's why we need each of them:

* We need the second one, `arg_0`, because it exists implicitly in the program.
  It's the unnamed thing being matched---in this case, checked to see if the
  value is 1.
* We have (but don't apparently need) the first one because in its un-optimized
  state, *every* function has a closure parameter. This is part of the "closure
  conversion" process whereby the abstract world-of-ideas notion of "function
  code + environment" gets turned (later) into "C code + tack-on struct". It
  makes functions callable with a very generic (slow) calling convention by
  default, but we might be able to optimize functions into a faster calling
  convention later in the project.

Then we have a new control instruction, `Jump`, that represents continuing the
flow of execution at another block. In this case, `bb2`.

`bb2` checks if `v1` is equal to the integer 1 and returns a native boolean
(not a Scrapscript object). This gets fed into our third control instruction so
far, `CondBranch`.

`CondBranch` takes a condition and two basic blocks. If the condition is true,
it passes control the first one. If it's false, to the second. This gives us
the ability to support conditionals in the source language.

In the true case, if the argument is 1, we add up 2 and 3 using the `IntAdd`
instruction. Otherwise, in `bb1`, we encounter our last control instruction:
`MatchFail`. This aborts the program.

### Cleaning up

You might notice that this IR snippet leaves something to be desired. For one,
we have an unconditional jump from `bb0` to `bb2` and `bb2` has no other
incoming control flow from any other blocks. It seems like we should be able to
smush the contents of `bb2` into `bb0` instead.

You might also notice that the closure parameter is completely unused. We
could get rid of it because nothing uses `v0`.

Last, you might notice that, similar to the first example with the list, we
have two constant operands to an instruction that seems like it should be able
to be computed at compile-time.

Some good news: all of these can be dealt with using very generic compiler
passes over the IR.

## Some optimization passes

Let's tackle each of the aforementioned concerns one by one.

To merge basic blocks, I adapted a compiler pass called CleanCFG from the
Cinder project, which I previously adapted from the compiler pass CleanCFG from
the HHVM project.

```python
@dataclasses.dataclass
class CleanCFG:
    fn: IRFunction

    def run(self) -> None:
        changed = True
        while changed:
            changed = False
            for block in self.fn.cfg.rpo():
                if not block.instrs:
                    # Ignore transient empty blocks.
                    continue
                # Keep working on the current block until no further changes are made.
                while self.absorb_dst_block(block):
                    pass
            changed = self.remove_unreachable_blocks()

    # ...
```

To remove dead code, we have a reasonably straightforward implementation of
dead code elimination (DCE). DCE is kind of like a garbage collector for
instructions: it finds the root set (the required instructions, kind of like
the backbone of the program), marks them, and then marks the instructions they
indirectly/transitively depend on. Then it removes (sweeps) the rest.

Because each instruction embeds a union-find data structure in it, I
"soft-delete" instructions by marking them equivalent to a no-op instruction. I
could have also gone through and deleted them, but deleting instructions from
the middle of list while iterating over it felt finicky and bothersome. `Nop`
instructions are not shown in the `to_string` representation of the IR and
don't compile to any C code.

```python
@dataclasses.dataclass
class DeadCodeElimination:
    fn: IRFunction

    def is_critical(self, instr: Instr) -> bool:
        if isinstance(instr, Const):
            return False
        if isinstance(instr, IntAdd):
            return False
        # TODO(max): Add more. Track heap effects?
        return True

    def run(self) -> None:
        worklist: list[Instr] = []
        marked: set[Instr] = set()
        blocks = self.fn.cfg.rpo()
        # Mark
        for block in blocks:
            for instr in block.instrs:
                instr = instr.find()
                if self.is_critical(instr):
                    marked.add(instr)
                    worklist.append(instr)
        while worklist:
            instr = worklist.pop(0).find()
            if isinstance(instr, HasOperands):
                for op in instr.operands:
                    op = op.find()
                    if op not in marked:
                        marked.add(op)
                        worklist.append(op)
        # Sweep
        for block in blocks:
            for instr in block.instrs:
                instr = instr.find()
                if instr not in marked:
                    instr.make_equal_to(Nop())
```

The last problem, the lack of constant folding, could be solved by a pass that
looks something like this:

```python
def simplify(fn):
    changed = True
    while changed:
        changed = False
        for block in fn.blocks:
            for instr in block.instrs:
                changed |= try_simplify(instr)
```

This would probably work, but it would do many passes over the entire control
flow graph for each function. For big functions with many small reduction
steps, this could take a long time.

Additionally, it doesn't include any type inference. Each instruction is
limited to the type information visible from its local context. For small
optimizations like our `2+3` or list cons, this might be fine, but there are
situations where local type information would not be enough to simplify
an instruction.

Instead, we crack open the [Constant propagation with conditional branches][sccp] paper (known informally as SCCP) which takes a smarter worklist approach.

[sccp]: https://dl.acm.org/doi/10.1145/103135.103136

Two worklists, actually. It uses one to hold basic blocks and one to hold
instructions. Starting at the entrypoint, it constructs a set of reachable
blocks and flows types (members of the constant propagation lattice) through
the instructions.

The exciting bit is that it uses the types in discovering the control flow: if
a conditional branch is known to branch a certain way at analysis time, the
other branch is not analyzed at all and its computation does not pollute the
analysis results.

Here is a snippet of SCCP, excluding the type lattice. The type lattice is an
interesting concept that I am implementing poorly and I would like to do a
better job (faster, smaller code, ...) later.

```python
@dataclasses.dataclass
class SCCP:
    fn: IRFunction
    instr_type: dict[Instr, ConstantLattice]
    block_executable: set[Block]
    instr_uses: dict[Instr, set[Instr]]

    def type_of(self, instr: Instr) -> ConstantLattice:
        ...

    def run(self) -> dict[Instr, ConstantLattice]:
        block_worklist: list[Block] = [self.fn.cfg.entry]
        instr_worklist: list[Instr] = []

        while block_worklist or instr_worklist:
            if instr_worklist and (instr := instr_worklist.pop(0)):
                instr = instr.find()
                if isinstance(instr, HasOperands):
                    for operand in instr.operands:
                        if operand not in self.instr_uses:
                            self.instr_uses[operand] = set()
                        self.instr_uses[operand].add(instr)
                new_type: ConstantLattice = CBottom()
                if isinstance(instr, Const):
                    value = instr.value
                    if isinstance(value, Int):
                        new_type = CInt(value.value)
                    if isinstance(value, List):
                        new_type = CList()
                # ...
                elif isinstance(instr, CondBranch):
                    match self.type_of(instr.operands[0]):
                        case CCBool(True):
                            instr.make_equal_to(Jump(instr.conseq))
                            block_worklist.append(instr.conseq)
                        case CCBool(False):
                            instr.make_equal_to(Jump(instr.alt))
                            block_worklist.append(instr.alt)
                        case CBottom():
                            pass
                        case _:
                            block_worklist.append(instr.conseq)
                            block_worklist.append(instr.alt)
                elif isinstance(instr, IntAdd):
                    match (self.type_of(instr.operands[0]), self.type_of(instr.operands[1])):
                        case (CInt(int(l)), CInt(int(r))):
                            new_type = CInt(l + r)
                            instr.make_equal_to(Const(Int(l+r)))
                        case (CInt(_), CInt(_)):
                            new_type = CInt()
                # ...
                old_type = self.type_of(instr)
                if union(old_type, new_type) != old_type:
                    self.instr_type[instr] = new_type
                    for use in self.instr_uses.get(instr, set()):
                        instr_worklist.append(use)
            if block_worklist and (block := block_worklist.pop(0)):
                if block not in self.block_executable:
                    self.block_executable.add(block)
                    instr_worklist.extend(block.instrs)

        return self.instr_type
```

One thing to note is that SCCP requires either pre-computing or incrementally
discovering *use edges* in your IR. That is, when it updates the type of
instruction *A*, it needs to know what other instructions use *A* so that they
can be re-queued for analysis.

Also please ignore the using-lists-as-queues thing which is terribly slow. You
should instead use a proper queue or deque structure.

## More advanced optimizations

Interprocedural

## Design decisions: what's up with SSA?

### SSI

## Compiling to C

Phi and parallel copy

## SSA vs CPS

## Testing
