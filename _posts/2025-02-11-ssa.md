---
title: A catalog of ways to generate SSA
---

[Static Single Assignment][wikipedia] is a program representation where each "variable"
(though this term can be misleading) is assigned exactly once. Mostly the
variables aren't variables at all, but instead names for values---for
expressions. SSA is used a lot in compilers. See [this lesson from
Cornell][cornell] for converting Bril to SSA, which also has some good
background. I'm making this page to catalog the papers I have found interesting
and leave a couple of comments on them.

[wikipedia]: https://en.wikipedia.org/wiki/Static_single-assignment_form
[cornell]: https://www.cs.cornell.edu/courses/cs6120/2022sp/lesson/6/

## A brief bit of background

It comes from the 1980s. Wikipedia claims the first paper about it (though not
by the name SSA) was [Code
Motion of Control Structures in High Level
Languages](/assets/img/zadeck-code-motion.pdf) (PDF), which I had not seen
before today. There was also [Global Value Numbers and Redundant
Computations](/assets/img/zadeck-gvn.pdf) (PDF), which I had also not seen
before today.

If you're confused about what a phi function is: they are annotations that join
dataflow edges from predecessor blocks into new variables. It's not a real
function, it doesn't (directly) generate any code, and it needs to be handled
differently than other instructions in your IR.

## Let's generate some variables

Finally, in 1991, the same group produced the first SSA paper I was already
familiar with: [The Cytron Paper](/assets/img/cytron-ssa.pdf) (PDF). This
approach requires computing dominance frontiers for an existing control-flow
graph, which may or may not be useful or applicable in your situation. If you
have bytecode, this might be workable, but you'll need to ["discover" the basic
blocks](/blog/discovering-basic-blocks/) hiding within. It's also a notable
paper because it produces the minimal amount of phi instructions.

In 1994, Brandis and Mössenböck write [Single-Pass Generation of Static
Single-Assignment Form for Structured
Languages](/assets/img/brandis-single-pass.pdf) (PDF). This is a neat approach
because it shows that you don't need to do Fancy Algorithms or Fancy Data
Structures to get SSA---you can build it as soon as during parsing, something I
have taken to doing recently. It turns out that if you don't have `goto`,
things get easier for the compiler developer.

In 1995, Richard Kelsey writes about how CPS and SSA are similar in [A
Correspondence between Continuation Passing Style and Static Single Assignment
Form](/assets/img/kelsey-ssa-cps.pdf) (PDF). Part of the paper involves
converting CPS to SSA (and the reverse, too).

Also in 1995, Cliff Click and Michael Paleczny publish [A Simple Graph-Based
Intermediate Representation](/assets/img/click-ssa.pdf) (PDF), which has become
known as the Sea of Nodes IR. It's like "normal" SSA except that instead of
just having blocks and data edges between instructions, all instructions are
"unscheduled" and have control edges between them. It means your graph looks
all funky but doesn't implicitly have (kind of arbitrary) code linearization so
code motion is easier. Cliff and co are building [a working demo
compiler](https://github.com/SeaOfNodes/Simple). See also [Global Code Motion /
Global Value Numbering](/assets/img/click-gvn.pdf) (PDF) by Cliff Click.

Also in 1995, Sreedhar and Gao write [A linear time algorithm for placing
ϕ-nodes](/assets/img/sreedhar-ssa.pdf) (PDF). This paper introduces a new data
structure, DJ graphs, that removes the need to compute iterated dominance
frontiers.

In 1996, Choi, Sarkar, and Schonberg write [Incremental Computation of Static
Single Assignment Form](/assets/img/incremental-ssa.pdf) (PDF) about restoring
SSA incrementally after rewriting instructions in compiler passes.

In 1998, Appel (of functional language fame) describes briefly a correspondence
between functional languages and SSA in [SSA is Functional
Programming](/assets/img/appel-basic-block-arguments.pdf) (PDF). I think this
is the first paper that introduced a notion of "basic block arguments" (instead
of phi functions). He also suggests a "really crude" algorithm for generating
SSA that places phi nodes all over the place for every variable.

Also in 1998, Briggs, Cooper, Harvey, and Simpson write [Practical Improvements
to the Construction and Destruction of Static Single Assignment
Form](/assets/img/briggs-ssa.pdf) (PDF), which I have minimal familiarity with
but builds semi-printed SSA.
Braun et al (see later) note that while this paper does not require liveness
information, it "can only present the creation of dead phi functions for
variables that are local to a basic block". I will also note that in the Briggs
paper the authors warn that it assumes "names are used a type-consistent
fashion" and for some reason this is crucial to their paper. I have not looked
very closely as to why.

In 2000, Aycock and Horspool publish [Simple Generation of Static
Single-Assignment Form](/assets/img/aycock-horspool-ssa.pdf) (PDF). It starts
from Appel's approach and then iteratively deletes phi nodes that don't need to
exist. They find that for reducible control-flow graphs (the common case for
most compilers, I think), their approach also produces minimal SSA.

In 2009, Michael Bebenita writes [Constructing SSA the Easy
Way](/assets/img/bebenita-ssa.pdf) (PDF) which is "essentially a rehashing of
Aycock's SSA construction algorithm but using forwarding pointers instead". I
only found it the other day. It's fantastic and I wish more people knew about
it. It uses one of my favorite data structures, union-find, though for some
reason it does not mention it by name.

Also in 2009, Paul Biggar and David Gregg [sketch an
algorithm](/assets/img/biggar-ssa.pdf) (PDF) for converting into SSA while
doing sparse conditional constant propagation (SCCP). Like Cytron's algorithm,
it requires having dominance information available. Per the paper, [the
implementation](https://github.com/pbiggar/phc/tree/96adc7bef3e222532196846c1f11ea4b2bc7930e/src/optimize/ssa)
appears to work but there are no proofs about it.

In some year between 2009 and 2018, Fil Pizlo creates and does not write much
about "Pizlo special" SSA[^pizlo-ssa]. It is used in DFG-SSA (high level in FTL
JIT) and B3 (low level in FTL JIT) in JavaScriptCore. After light prodding on
Hacker News, he writes [this
gist](https://gist.github.com/pizlonator/79b0aa601912ff1a0eb1cb9253f5e98d)
(mirrored in footnote) and a [longer
explanation](https://gist.github.com/pizlonator/cf1e72b8600b1437dda8153ea3fdb963)
of the full IR.

[^pizlo-ssa]: (from Fil Pizlo, in the first person)

    This describes how I do SSA form, which avoids the need to have any
    coupling between CFG data structures and SSA data structures.

    Let's first define a syntax for SSA and some terminology. Here's an example
    SSA node:

        A = Add(B, C)

    In reality, this will be a single object in your in-memory representation,
    and the names are really addresses of those objects. So, this node has an
    "implicit variable" called A; it's the variable that is implicitly assigned
    to when you execute the node. If you then do:

        X = Sub(A, 1)

    Then "A" is just a pointer to the Add node, and we're using the implicit
    variable "A".

    Here's an example function:

        int foo(int a, int b)
        {
            int x;
            if (a)
                x = b + 1
            else
                x = b * 2
            return x + 42;
        }

    Here's an SSA program with a Phi in Pizlo form:

        root:
            A = GetArgument(0)
            B = GetArgument(1)
            Branch(A, then, else)
        then:
            X1 = Add(B, 1)
            Upsilon(X1, ^X)
            Jump(return)
        else:
            X2 = Mul(B, 2)
            Upsilon(X2, ^X)
            Jump(return)
        return:
            X = Phi()
            R = Add(X, 42)
            Return(R)

    In Pizlo form:

    - Every SSA node has an implicit variable, as mentioned above.

    - Every Phi node has a shadow variable in addition to the implicit variable.

    Let's say that given a Phi like "X = Phi()", the implicit variable is
    called "X", and the shadow variable is called "^X".

    Therefore, the semantics of an upsilon like "Upsilon(X1, ^X)" is just "set
    ^X = X1". And the semantics of a Phi like "X = Phi()" is just "set X = ^X".

    In other words, you can think of Upsilon as being a side effect (a store to
    a shadow variable). And you can think of Phi as being a side effect (a load
    from a shadow variable). You can model them that way in your effect
    analysis to block reordering Upsilons and Phis.

    But also, the shadow variables of Phis in Pizlo form are "Static Single
    Use" (SSU) variables. This falls out naturally from the fact that the only
    syntax for loading a shadow variable is the Phi itself. So you can think of
    Pizlo form as "SSA-SSU form".

    The main benefit of this form is that basic blocks - and all CFG data
    structures - have zero knowledge about SSA. There are no basic block
    arguments. There's no requirement that Phis appear at the tops of blocks.
    In fact, this is a valid program in Pizlo form (albeit suboptimal):

        M = Stuff(...)
        Upsilon(M, ^N)
        N = Phi()
        MoreStuff(N)

    Here, there's a Phi in them middle of a basic block, and there's an Upsilon
    just before it. That's fine. This is important, because it means that you
    can do CFG transforms that blow away control flow edges without worrying
    about fixing your Phis.

    In any Pizlo-form compiler, you'll want to have a Phi simplification pass,
    which you can implement either by running Cytron or by running any other
    SSA converter. The simplest is just to just fixpoint the rule that if you
    have a Phi that has Upsilons that only use the Phi or exactly one other
    value, then replace the Phi with that other value.

In 2013, my former coworker Matthias Braun and his labmates write [Simple and
Efficient Construction of Static Single Assignment
Form](/assets/img/braun13cc.pdf) (PDF), which describes converting to SSA from
an AST and building the CFG on the fly. It's fairly popular because it is
simpler than the Cytron paper. I find the phase transitions in the blocks
(filled/sealed/...) a little tricky to keep straight though.

In 2020, Abu Naser Masud and Federico Ciccozzi write [More precise construction
of static single assignment programs using reaching
definitions](/assets/img/masud-ssa.pdf) (PDF), which uses reaching definitions
instead of dominance frontiers. They find it generates fewer superfluous (?)
phi nodes than the DF-based approach and takes only ~2x the time.

In 2023, Matthieu Lemerre writes [SSA Translation Is an Abstract
Interpretation](/assets/img/lemerre-ssa.pdf) (PDF), which I would love to
understand one day.

## Other papers

I will eventually add some papers on extensions to SSA, analyses on SSA,
converting out of SSA (including register allocation). Just not this evening.

Some papers for out-of-SSA:

* [Revisiting Out-of-SSA Translation for Correctness, Code Quality, and Efficiency](/assets/img/boissinot-out-ssa.pdf) (PDF)

I am also probably missing or forgetting some big hit papers for converting
into SSA, so please drop me a line if you have favorites.

## Other resources

The [SSA book](/assets/img/ssa-book.pdf) (PDF; draft) is a huge tour de force
of SSA-based compiler design. That is even the new name of the book, which has
since been [published in
print](https://link.springer.com/book/10.1007/978-3-030-80515-9). It's on my
shelf.

## A keyword dump

Here are some keywords which you may search for but mostly serve as notes to
self to write more about one day:

* windmill, lost copy
* SCCP
* liveness
* DCE
* chordal
* https://arxiv.org/abs/2011.05608
* SSI and e-SSA and pi nodes and sparseness
* GVN and CSE and hash consing
* load/store forwarding
* mem2reg
* SROA and memory ssa
* eager constant folding / smart constructors
* points-to and CFA
* flattening tuples
* webs (see Muchnick's 1997 Advanced compiler design and implementation)
* RVSDG and co
* e-graphs
* https://www.dcs.gla.ac.uk/~jsinger/ssa.html
