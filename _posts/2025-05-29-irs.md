---
title: "What I talk about when I talk about IRs"
layout: post
---

I have a lot of thoughts about the design of compiler intermediate
representations (IRs). In this post I'm going to try and communicate some of
those ideas and why I think they are important.

The overarching idea is *being able to make decisions with only local
information.*

That comes in a couple of different flavors.

## Register-based IRs

Some IRs are stack based. For concatenative languages or some newer JIT
compilers, IRs are formatted in such a way that each opcode reads its operands
from a stack and writes its outputs to a stack. This is reminiscent of a
point-free coding style in languages such as Haskell or OCaml.

```haskell
inc = (+ 1)
```

In this style, there is an implicit shared state: the stack. Dataflow is
explicit (pushes and pops) and instructions can only be rearranged if the stack
structure is preserved. This requires some non-local reasoning: to move an
instruction, one must also rearrange the stack.

```
PUSH 1
PUSH 2
ADD
```

By contrast, in a register-based IR, things are more explicit. Instructions
take named inputs (`v0`, `v12`, etc) and produce named outputs. Instructions
can be freely moved (modulo effects) as long as inputs remain defined. The
stack does not exist.

```
v1 = CONST 1
v2 = CONST 2
v3 = ADD v1 v2
```

The constraints (names being defined) are *part of the IR*. This gets a little
bit tricky if it's possible to define a name multiple times.

```
v1 = CONST 1
v2 = CONST 2
x = ADD v1 v2
x = ADD x v2
v3 = MUL x 12
```

What does `x` mean in the instruction for `v3`? Which definition does it refer
to? In order to reason about the instruction `v3 = MUL x 12`, we have to keep
around some context. Unless we decide to enforce some interesting rules...

<!--
If some analysis pass
tells us that `x` has some value or some type, it may be tricky to
disambiguate. Let's take this locality property a little bit further.
-->

## SSA

Static single assignment (SSA) was introduced by [...] in TODO (see [my blog
post](/blog/ssa/) about the different implementations). In SSA-based IRs, each
variable can only be defined once. Put another way, a variable *is* its
defining instruction; alternately, a variable and its defining instruction are
addressed by the same name. The previous example is not valid SSA; `x` has two
definitions.

If we turn the previous example into SSA, we can now use a different name for
each instruction. This is related to the [unique name assumption][] or the
global names property: names do not depend on context.

[unique name assumption]: https://en.wikipedia.org/wiki/Unique_name_assumption

```
v1 = CONST 1
v2 = CONST 2
x0 = ADD v1 v2
x1 = ADD x0 v2
```

Now we can identify each different `ADD` instruction by the variable it
defines. This is useful in analysis...

<!--
if an analysis tells us that the value of
`x0` is `5`, we can use that property throughout the program, completely
independent of the context (more in the next section).

SSA makes writing analyses and interpreting analyses easier. Great. But we
don't want to always do transformations inside the analysis pass; sometimes we
want to separate the phases. So I advocate for storing analysis results on the
IR nodes.
-->

## Type information

I took a class with Olin Shivers and he described abstract interpretation as
"automated theorem finding". Unlike theorem *provers* such as Lean and Rocq,
where you have to manually prove the properties you want, static analysis finds
interesting facts that already exist in your program (and optimizers use them
to make your program faster).

Your static analysis pass(es) can annotate your IR nodes with little bits of
information such as:

* has type `int`
* has value `5`
* is between `2` and `5`
* is equivalent to the expression `y*z`
  * https://cfbolz.de/posts/record-known-result/
* does not have side effects
* is loop invariant
* does not escape
* ...

If your static analysis is over SSA, then generally the static analysis is
easier and (potentially) storing facts is easier. This is due to this property
called *sparseness*. Where a static analysis over non-SSA programs has to store
facts about all variables at *all program points*, an analysis over SSA need
only store facts about all variables, independent of context.

I sometimes describe this as "pushing time through the IR" but I don't know
that that makes a ton of sense.

But SSA only stores type information about instructions and does not encode
information that we might later learn in the IR. With basic SSA, there's not a
good way to encode refinements...

## SSI

Static single information (SSI) form gives us new ways to encode metadata about
instructions (variables). It was introduced by Ananian in [...] TODO. Consider
the following SSA program (represented as pseudo-Python):

```python
# @0
x = int(...)
# @1
if x >= 0:
    # @2
    return x
else:
    # @4
    result = -x
    # @5
    return result
```

`x` is undefined at `@0`. `x` is defined *and an integer* at `@1`. But then we
do something interesting: we split control flow based on the run-time value of
`x`. We can take this split to add new and interesting information to `x`. For
non-sparse analysis, we can record some fact on the side. That's fine.

```python
# @0
x = int(...)
# @1
if x >= 0:
    # @2
    LearnFact(x, nonnegative)
    # @3
    return x
else:
    # @4
    LearnFact(x, negative)
    # @5
    result = -x
    # @6
    return result
```

When doing a dataflow analysis, we can keep track of the fact that at `@3`, `x`
is nonnegative, and at `@5`, `x` is negative. This is neat: we can then
determine that all paths to this function return a positive integer.

If we want to keep our information sparse, though, we have to add a new
definition to the IR.

```python
# @0
x = int(...)
# @1
if x >= 0:
    # @2
    newx0 = LearnFact(x, nonnegative)
    # @3
    return newx0
else:
    # @4
    newx1 = LearnFact(x, negative)
    # @5
    result = -newx1
    # @6
    return result
```

This is complicated (choose which variables to split, replace all uses, to
maintain SSA, etc) but gives us new places to store information *inside the
IR*. It means that every time we refer to `newx0`, we know that it is
nonnegative and every time we refer to `newx1`, we know that it is negative.
This information is independent of context!

I should note that you can get a lot of the benefits of SSI without going "full
SSI". There is no need to split every variable at every branch, nor add a
special new merge instruction.

Okay, so we can encode a lot of information very sparsely in the IR. That's
neat. It's powerful. But we should also be mindful that even in this very
sparse representation, we are encoding information implicitly that we may not
want to: execution order.

## Sea of nodes

In a control-flow graph (CFG) representation, the instructions are already
*scheduled*, or ordered. Normally this order comes from the order given by the
programmer in the original source form.

Instead of including metadata, also be mindful of what you encode implicitly
(e.g. with order) and instead encode it explicitly as effect ordering edges

## E-graphs
