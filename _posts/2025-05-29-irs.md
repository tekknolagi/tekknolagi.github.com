---
title: "What I talk about when I talk about IRs"
layout: post
---

I have a lot of thoughts about the design of compiler intermediate
representations (IRs). In this post I'm going to try and communicate some of
those ideas and why I think they are important.

The overarching idea is *being able to make decisions with only local
information.*

That comes in a couple of different flavors.

## Register-based IRs

Some IRs are stack based. For concatenative languages or some newer JIT
compilers, IRs are formatted in such a way that each opcode reads its operands
from a stack and writes its outputs to a stack. This is reminiscent of a
point-free coding style in languages such as Haskell or OCaml.

```haskell
inc = (+ 1)
```

In this style, there is an implicit shared state: the stack. Dataflow is
explicit (pushes and pops) and instructions can only be rearranged if the stack
structure is preserved. This requires some non-local reasoning: to move an
instruction, one must also rearrange the stack.

```
PUSH 1
PUSH 2
ADD
```

By contrast, in a register-based IR, things are more explicit. Instructions
take named inputs (`v0`, `v12`, etc) and produce named outputs. Instructions
can be freely moved (modulo effects) as long as inputs remain defined. The
stack does not exist.

```
v1 = CONST 1
v2 = CONST 2
v3 = ADD v1 v2
```

The constraints (names being defined) are *part of the IR*. This gets a little
bit tricky if it's possible to define a name multiple times.

```
v1 = CONST 1
v2 = CONST 2
x = ADD v1 v2
x = ADD x v2
v3 = MUL x 12
```

What does `x` mean in the instruction for `v3`? Which definition does it refer
to? In order to reason about the instruction `v3 = MUL x 12`, we have to keep
around some context. Unless we decide to enforce some interesting rules...

<!--
If some analysis pass
tells us that `x` has some value or some type, it may be tricky to
disambiguate. Let's take this locality property a little bit further.
-->

## SSA

In static single assignment (SSA) based IRs, each variable can only be defined
once. Put another way, a variable *is* its defining instruction; alternately, a
variable and its defining instruction are addressed by the same name. The
previous example is not valid SSA; `x` has two definitions.

If we turn the previous example into SSA, we can now use a different name for
each instruction. This is related to the [unique name assumption][] or the
global names property: names do not depend on context.

[unique name assumption]: https://en.wikipedia.org/wiki/Unique_name_assumption

```
v1 = CONST 1
v2 = CONST 2
x0 = ADD v1 v2
x1 = ADD x0 v2
```

Now we can identify each different `ADD` instruction by the variable it
defines. This is useful in analysis: if an analysis tells us that the value of
`x0` is `5`, we can use that property throughout the program, completely
independent of the context. I sometimes describe this as "pushing time through
the IR" but I don't know that that makes a ton of sense.

SSA makes writing analyses and interpreting analyses easier. Great. But we
don't want to always do transformations inside the analysis pass; sometimes we
want to separate the phases. So I advocate for storing analysis results on the
IR nodes.

## Type information

I took a class with Olin Shivers and he described abstract interpretation as
"automated theorem finding". Unlike theorem *provers* such as Lean and Rocq,
where you have to manually prove the properties you want, optimizers find
interesting facts that already exist in your program (and use them to make your
program faster).

This involves annotating your IR nodes with little bits of information such as:

* has type `int`
* has value `5`
* is between `2` and `5`
* is equivalent to the expression `y*z`
* does not have side effects
* is loop invariant
* ...

## SSI

Path-dependent information
Flow typing
Refinements

## Sea of nodes

Instead of including metadata, also be mindful of what you encode implicitly
(e.g. with order) and instead encode it explicitly as effect ordering edges
